{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk2R_QYyXffm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "from skimage import exposure, filters\n",
        "from google.colab import drive\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "image_folder = '/content/drive/My Drive/IMAGE CLASSIFICATION PNEUMONIA/'\n",
        "csv_file_path = '/content/drive/My Drive/IMAGE CLASSIFICATION PNEUMONIA/Images Label.csv'\n",
        "\n",
        "# Load and preprocess images\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
        "    if img is None:\n",
        "        return None\n",
        "\n",
        "    img = cv2.resize(img, target_size)\n",
        "\n",
        "    # Apply CLAHE\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img = clahe.apply(img)\n",
        "\n",
        "    # Apply Contrast Enhancement (Gamma Correction)\n",
        "    gamma = 1.2  # Tunable hyperparameter\n",
        "    img = exposure.adjust_gamma(img, gamma)\n",
        "\n",
        "    # Apply Otsu's Thresholding\n",
        "    threshold = filters.threshold_otsu(img)\n",
        "    img = (img > threshold).astype(np.uint8) * 255\n",
        "\n",
        "    return img\n",
        "\n",
        "# Load dataset\n",
        "def load_images_from_csv(csv_df, image_folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_paths = [os.path.join(root, file) for root, _, files in os.walk(image_folder) for file in files if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for _, row in csv_df.iterrows():\n",
        "        image_name = row['Image Index']\n",
        "        label = row['Label']\n",
        "        matching_path = next((path for path in image_paths if os.path.basename(path) == image_name), None)\n",
        "\n",
        "        if matching_path:\n",
        "            preprocessed_img = preprocess_image(matching_path)\n",
        "            if preprocessed_img is not None:\n",
        "                images.append(preprocessed_img)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images and labels\n",
        "labels_df = pd.read_csv(csv_file_path)\n",
        "labels_df = labels_df[labels_df['Finding Labels'].str.contains('Pneumonia', case=False, na=False)]\n",
        "images, labels = load_images_from_csv(labels_df, image_folder)\n",
        "\n",
        "# Feature Extraction using PCA\n",
        "def extract_features(images, n_components=100):\n",
        "    images_flattened = images.reshape(images.shape[0], -1)  # Flatten images\n",
        "    pca = PCA(n_components=n_components)  # Reduce to 100 principal components\n",
        "    images_pca = pca.fit_transform(images_flattened)\n",
        "    return images_pca\n",
        "\n",
        "features = extract_features(images, n_components=100)\n",
        "labels = labels.ravel()  # Ensure labels are 1D\n",
        "\n",
        "print(\"New feature shape:\", features.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "# Step 1: Clustering to create Local Regions\n",
        "n_clusters = 3  # Choose based on data complexity\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(features)\n",
        "\n",
        "# Step 2: Train a model per cluster\n",
        "def create_model(input_shape=(100,), dropout_rate=0.3, learning_rate=1e-3):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512)(inputs)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(256)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(128)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "    model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "    return model\n",
        "\n",
        "# Train separate models for each cluster\n",
        "local_models = {}\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    print(f\"Training model for cluster {cluster_id}...\")\n",
        "\n",
        "    # Extract subset\n",
        "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "    X_cluster = features[cluster_indices]\n",
        "    y_cluster = labels[cluster_indices]\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_cluster, y_cluster, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model = create_model(input_shape=(100,))\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=0)\n",
        "\n",
        "    # Store trained model\n",
        "    local_models[cluster_id] = model\n",
        "\n",
        "print(\"Training for all clusters complete.\")\n",
        "\n",
        "# Step 3: Inference - Dynamic Model Selection\n",
        "def predict_with_ensemble(test_features):\n",
        "    cluster_assignments = kmeans.predict(test_features)\n",
        "    predictions = []\n",
        "\n",
        "    for i, cluster_id in enumerate(cluster_assignments):\n",
        "        model = local_models[cluster_id]\n",
        "        y_pred_prob = model.predict(test_features[i].reshape(1, -1))\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "        predictions.append(y_pred[0])\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Evaluate final ensemble model\n",
        "y_pred = predict_with_ensemble(features)\n",
        "precision = precision_score(labels, y_pred, zero_division=1)\n",
        "recall = recall_score(labels, y_pred, zero_division=1)\n",
        "f1 = f1_score(labels, y_pred, zero_division=1)\n",
        "\n",
        "print(f\"Final Model Precision: {precision:.4f}\")\n",
        "print(f\"Final Model Recall: {recall:.4f}\")\n",
        "print(f\"Final Model F1-Score: {f1:.4f}\")\n"
      ]
    }
  ]
}